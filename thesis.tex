%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% EPFL report package, main thesis file Goal: provide formatting for theses and
% project reports Author: Mathias Payer <mathias.payer@epfl.ch>
%
% This work may be distributed and/or modified under the conditions of the
% LaTeX Project Public License, either version 1.3 of this license or (at your
% option) any later version.  The latest version of this license is in
% http://www.latex-project.org/lppl.txt
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% useful links:
% REPICA: https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8454360
%         good state of the art overview, both ARM64 and arm32
%         1. do not use symbolization, but relative address correction
%         2. Detect jump tables and global pointers, but not fix them
%         3. Overhead is exactly the same as retrowrite


%\pdfcompresslevel=0 \pdfobjcompresslevel=0

\documentclass[a4paper,11pt,oneside]{report}
\usepackage[MScThesis,lablogo]{EPFLreport} \usepackage{xspace}
\usepackage{xcolor} 
\usepackage{soul}
\usepackage{hyperref}
\usepackage{listings} 

\definecolor{lgray}{rgb}{0.92,0.92,0.92}

\lstset{ %
  language=[x86masm]Assembler,    
  framexleftmargin=5pt,
  framexrightmargin=5pt,
  %framesep=100pt,
  basicstyle=\large,      
  numbers=left,                  
  numberstyle=\small\color{gray},  
  numbersep=15pt,
  stepnumber=1,                   
  backgroundcolor=\color{lgray},  
  tabsize=4,                      
  captionpos=b,                   
  breaklines=true,               
  breakatwhitespace=false,      
  title=\lstname,              
  keywordstyle=\color{blue},     
  commentstyle=\color{dkgreen}, 
  stringstyle=\color{mauve},   
  escapeinside={\%*}{*)},     
  morekeywords={adrp, add, adr, ldrb, br, uxtw, sxtb}
}


\title{Arm Wrestling: porting the Retrowrite project to the ARM architecture}
\author{Luca Di Bartolomeo} \adviser{Prof. Mathias Payer (EPFL)}
\supervisor{Prof. Kenny Paterson (ETH)}

\linespread{1.5} 
\newcommand{\sysname}{RetroWrite\xspace}

%\newcommand{\todo}[1]{\colorbox{cyan}{\parbox{0.9\textwidth}{#1}}}
\newcommand{\todo}[1]{%
	\begingroup 
	\sethlcolor{cyan}%
	\hl{TODO: #1}%
	\endgroup
}


\dedication{ 
\begin{raggedleft}
	No matter where you go, everyone is connected.\\
	--- Serial Experiments Lain\\
\end{raggedleft} 
\vspace{4cm} 
\begin{center}
	Dedicated to my parents, my sister Sara, my dear Giulia, to my friends back 
	in Rome and to my roommates Matteo and Filippo who all inspired me and kept 
	up with my constant complaining. Thanks!
\end{center} 
}

\acknowledgments{
	I would like to thank my advisor, Prof. Mathias Payer, for his support,
	guidance, and for trusting me by assigning me this inspiring project.  I
	admire him a lot and I wish all the best for him, and in particular I hope
	that I can work on many other projects with him.
	
	I would also like to thank his research group, HexHive, as I always found
	myself very welcome there, even if I could visit them only once a week.  
	Those have been six very happy months in which I learned quite a lot of 
	things, and I have to thank prof. Prof. Payer and his doctorate students 
	and researches for it. I wish them all to have a very succesfull career, 
	and I hope we can continue to work together in the future!
	
	Special thanks goes to my family and my friends in Rome. Their support was
	always available, and it has always been a huge pleasure to visit them once
	in a while in Italy. I also need to thank my S.O. Giulia, I felt she was
	always behind my back, keeping a good check on my mental sanity during the
	worst times of the outbreak. My roommates too, Matteo and Filippo, deserve
	a mention here, as their patience and their rubber duck debugging skills
	proved to be fundamental during some nasty debugging sessions.

	Finally, I would also like to mention my CTF team, Flagbot, that made me 
	spend so many weekends without going out but ultimately lead me to meet so 
	many new interesting people, and the teams I had played with occasionally, 
	namely polyglots and TRX. Thanks!

}

\begin{document}
\maketitle
\makededication
\makeacks


\begin{abstract}

	While there were good recent attempts, 
	\footnote{\url{https://www.pine64.org/pinephone/}}
	\footnote{\url{https://system76.com/}}
	\footnote{\url{https://puri.sm/products/}}
	limiting our computing to only open-source software
	is particularly hard, and especially on the mobile phone market even the
	most determined users are often forced to use closed-source libraries or
	modules.  Those often run at privileges higher than we might want (e.g.,
	manufacturer specific kernel modules \cite{androidclosed}), and are also
	hard to audit for vulnerabilities. 

	Many existing tools were developed to improve the auditability of closed
	source programs, especially aimed at helping the fuzzing process, with
	approaches such as implementing AddressSanitizer (a compiler pass only
	available with the source code) through dynamic instrumentation. However,  
	even state-of-the-art dynamic instrumentation engines incur in prohibitive 
	runtime overhead (10x and more). 

	In this thesis, we would like to show that symbolization for ARM binaries 
	is a viable alternative to existing approaches, that has less flexibility 
	(only works on C, position independent binaries) but has
	negligible overhead compared to compiled source code. We present the ARM
	port of \sysname, an existing static binary rewriter for \texttt{x86}
	executables, which implements the symbolization engine and the memory
	sanitization instrumentation. 

	While \sysname is not the only binary rewriter for \texttt{x86}, there are 
	very few that are aimed at the ARM64 architecture. In particular, most of 
	them resort to lifting to an intermediate IR (achieving more flexibility 
	but losing fine-grained control over the instrumented instructions) or use 
	approaches like trampolines which make the whole problem much easier but 
	also introduce a noticeable overhead.
	\todo{add a conclusion to this abstract, like "our proof of work 
	implementation of retrowrite is xx\% faster than QEMU and etc..."}


\end{abstract}


\maketoc

%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
%%%%%%%%%%%%%%%%%%%%%%
%The introduction is a longer writeup that gently eases the reader into your
%thesis~\cite{dinesh20oakland}. Use the first paragraph to discuss the setting.
%In the second paragraph you can introduce the main challenge that you see.
%The third paragraph lists why related work is insufficient.
%The fourth and fifth paragraphs discuss your approach and why it is needed.
%The sixth paragraph will introduce your thesis statement. Think how you can
%distill the essence of your thesis into a single sentence.
%The seventh paragraph will highlight some of your results
%The eights paragraph discusses your core contribution.
%This section is usually 3-5 pages.

% First paragraph: the setting
Security researchers routinely find vulnerabilities in major desktop OSes, but 
recently a lot of focus has shifted on the analysis of mobile operating 
systems, including Android.  More than 40 CVEs for Android were published just 
in December of 2019 
\footnote{\url{https://source.android.com/security/bulletin/2019-12-01}}.  Such 
big focus on the analysis of Android has certainly a lot to do on with the rise 
of the mobile phone market in the latest years, but also with the fact that the 
source code of Android is (mostly) open and available for everyone to dig 
through and debug.

Still, digging through the code is not the only way of auditing a codebase, in 
fact a very effective technique as of today is the process of \emph{fuzzing}.  
It is taking a lot of traction because setting up a fuzzing campaign, even a 
large scale one, is relatively easy for beginners too. The main advantages of 
fuzzing compared to other automatic vulnerability discovery techniques is that 
it is not needed to have a deep knowledge of the fuzzer engine to have a 
successful run, it is only required to select the initial input seed corpus, 
and to implement a fuzzing harness. Moreover, the approach is extremely easy to 
parallelize, so it is not uncommon for companies to setup large server farms 
with thousands of CPU cores dedicated to fuzzing.

Having the source code available while fuzzing immensely helps the fuzzer 
engine thanks to coverage information, with which the fuzzer can prune inputs 
much more accurately compared to \emph{black-box} fuzzing.  Furthermore, the 
presence of source code gives the opportunity to apply instrumentation passes, 
which can help the fuzzer even more.  For instance, memory sanitization helps 
the fuzzer find memory corruption bugs as soon as they are introduced without 
needing to wait for the target to crash. 

Unfortunately, in spite of the increasing popularity of open source software, 
closed source modules and binaries are still much more common: on desktop, 90\% 
of the operating systems are closed source, and even who runs an open-source 
alternative usually still has many closed source components (e.g., Skype, 
Nvidia drivers, various DRM components in browsers, etc.). This holds true even 
in the phone market: while 75\% \cite{statista} of mobile users use Android or 
other open-source operating systems, virtually all of those devices have 
proprietary components already installed at the moment of purchase. Most 
commonly there are Google proprietary libraries like the Play API, manufacturer 
custom modules and firmwares (for hardware peripherals such as Bluetooth or 
WWAN connectivity), and much more. In many instances, those closed-source 
modules are installed as kernel modules or run with root privileges. To make 
things worse, they are often exposed to the network, creating opportunities for 
malware or other bad actors to take control of mobile devices.

When considering fuzzing closed source binaries, the main difficulties that 
arise are due to the fact that the compilation process discards a large amount 
of information that could be very useful while hunting bugs, such as type 
information, the control flow graph, array boundaries, distinction between 
scalars/references, and much more.  Still, there are techniques that can help 
instrument a binary and fuzz it in those conditions, and they all fall into a 
common category called \emph{Binary rewriting}.

Binary rewriting techniques can be divided into two main approaches: 
\emph{dynamic} rewriting and \emph{static} rewriting. Dynamic rewriting 
involves running the target binary in a controlled execution alongside the 
rewriter engine, which instruments the target binary on the fly. The advantage 
of this approach is that at runtime the rewriter is able to recover some of the 
information that was lost during compilation (the most important one being the 
distinction between scalars and references), and so do not need to rely on 
heuristics, and can support a very wide variety of binaries (even packed or 
self-modifying binaries). However, the trade-off is the overhead introduced in 
the process: dynamic rewriters usually have between 10x and 100x of overhead 
\cite{dinesh20oakland}, which has a great impact on fuzzing performance. 

On the other hand, static rewriting is the opposite approach: instrument the 
target binary ahead of time, using various techniques to recover as much 
information as possible from the code (including heuristics). Flexibility of 
static rewriters is more limited, as virtually none of them support edge cases 
such as self-modifying binaries, code that does not support standard control 
flow practices (like OCaml or Haskell binaries), or even code generated by a 
non-popular compiler. However, the overhead introduced by static rewriting is 
generally very low, and moreover it can support more complex instrumentation 
passes (while dynamic rewriters need to keep it simple for performance 
constraints). 

Since static rewriters lack flexibility, the aim of this thesis is to expand 
the domain of supported binaries of one of the recently published static 
rewriters, \sysname, by Dinesh et al. \cite{dinesh20oakland}. Retrowrite uses 
the relatively new approach of \emph{symbolization}, that produces 
reassemblable assembly on which instrumentation passes may be applied to.  
\sysname focuses on position-independent binaries to eliminate the problem of 
distinguishing between scalars and pointers, and manages to produce 
reassemblable assembly without the usage of heuristics. 

\sysname supports only \texttt{x86\_64} binaries in both user and kernel 
space; however, most mobile devices are running on an ARM CPU, and also some 
new laptop devices like the latest line of MacBooks are switching to ARM. In 
this thesis we will explain our efforts to add support for userland ARM64 
binaries, with the goal that one day we will be able to fuzz Android modules 
and closed source components. Furthermore, we will also explain how we ported 
to ARM the original instrumentation pass that the authors of \sysname 
implemented, \emph{AddressSanitizer}, as a proof of concept to show the 
usefulness of the symbolization approach. To the best of our knowledge, there 
were no previous attempts at statically introducing memory sanitization to ARM 
binaries in the past. 

We will show through benchmarks how the overhead of \sysname's 
instrumentation is very low when compared to source based instrumentation, and 
much lower than state-of-the-art dynamic instrumentation. In our experiments, 
we find that we are \todo{xx\%} slower than source based instrumentation, and 
\todo{xx\%} faster than state-of-the-art dynamic instrumentation.

Our core contributions consist in solving many challenges that static rewriting 
ARM binaries present, mostly spawning from the fact that ARM is a fixed size 
instruction set, in which pointers need to be built in multiple instructions, 
or located in memory in a special region called a \emph{literal pool}. A series 
of new problems like jump table size fixing arise from this, and, to the best 
of our knowledge, some of those were never analyzed before.  We present our 
approach to fix or workaround those problems, and will give pointers to a few 
alternative solutions we analyzed.

We then evaluate the performance of our ARM port of \sysname , discuss its 
limitations, discuss related work, and present our conclusions.




% Second paragraph: main challenge

% Third paragraph: related work is insufficient, and why

% Fourth+Fifth paragraph: our approach, why is it needed

% Sixth paragraph: thesis statement (single sentence?)

% Seventh paragraph: highlight of results

% Eight paragraph: our core contribution

%%%%%%%%%%%%%%%%%%%%
\chapter{Background}
%%%%%%%%%%%%%%%%%%%%
%The background section introduces the necessary background to understand your
%work. This is not necessarily related work but technologies and dependencies
%that must be resolved to understand your design and implementation.
%This section is usually 3-5 pages.

Here we will provide a summary of the basic notions required to understand the 
underlying concepts behind \sysname and the problems we faced while porting 
it to the ARM architecture. 

\section{Binary Rewriting}
\emph{Binary rewriting} describes the alteration of a compiled program without 
having the source code at hand in such a way that the binary under 
investigation keeps the same functionality. The applications of binary 
rewriting are multiple and can be summarized as follows:
\begin{itemize}
	\item \textbf{Emulation}: An emulator is a special software that mimics the 
		behaviour of a device while executing on a different device. Emulators 
		use binary rewriting to translate system calls, instructions, memory 
		access and all the other execution primitives from one processor 
		architecture to another.  An example of this would be QEMU \cite{qemu}.

	\item \textbf{Optimization}: In high performance computing domains, having 
		a way to patch subtle things like cache misses or timing anomalies in 
		very long running tasks without the need to restart the whole program 
		is of special interest. In such situations, binary rewriting is a 
		solution for run-time patching, as it is done by DynInst \cite{dyninst} 
		or Frida \footnote{\url{https://frida.re/}}

	\item \textbf{Observation}: Having an in-depth look during the execution of 
		a binary by inserting profiling or tracing instructions in the middle 
		of its code can prove to be particularly useful in many applications, 
		like catching memory leaks (e.g., Valgrind \cite{valgrind}), coverage 
		information for fuzzing (e.g., AFL-QEMU \cite{afl}) and more.

	\item \textbf{Hardening}: The absence of source code, combined with no 
		vendor support, or deprecated build tools, could make the use of binary 
		rewriting necessary to introduce security measures such as the 
		insertion of stack canaries (e.g., Stackguard \cite{stackguard}) or 
		memory sanitization (e.g., \sysname \cite{dinesh20oakland}).
	
\end{itemize}

Instrumenting a binary is not an easy task, as it is often said that binaries 
have a \emph{rigid} structure, because moving any data or code, or inserting 
instructions, will break all the references inside said binary. Thus, a binary 
rewriter must make sure that all the pointers and control flow broken by the 
inserted instrumentation are corrected.

In the next section we will analyze the difference between the two different 
approaches that can be taken for binary rewriting, namely \emph{dynamic} and 
\emph{static} instrumentation.




\section{Dynamic and Static Instrumentation}

\subsection{Dynamic instrumentation}
Dynamic rewriters modify and instrument the code of the target binary during 
runtime. Usually, the target binary is executed in a controlled environment 
side by side with the rewriter engine, which patches instructions and fixes 
references on the go. Sometimes the rewriter engine leverages the operating 
system's primitives to control the execution of the target, like using the 
\texttt{ptrace} system call on Linux, but there are notable cases in which the 
rewriter engine comes with their own instrumentation runtime (e.g., Dynamo 
\cite{dynamo}) or implement a full featured virtual machine (e.g., STRATA 
\cite{strata}).

The big advantage of dynamic rewriters is the additional information that is 
only available at run time, like the path that the execution has taken.  
Furthermore, dynamic rewriters can avoid analyzing the whole binary at once, as 
they can just focus on the part that is being currently executed, making them 
scalable to arbitrarily large programs.

However, the additional runtime information comes at a high performance cost: 
running the rewriter engine alongside the target binary is expensive, and 
furthermore the frequent context switches the CPU must do to execute both make 
the situation even worse. The total overhead for an instrumentation pass like 
memory sanitization for a state-of-the-art dynamic rewriter like Valgrind are 
in the order of 10x \cite{dinesh20oakland} the overhead introduced by 
source-level memory sanitization .

\subsection{Static instrumentation}
Static rewriters process the target binary \emph{before} execution, and produce 
as output a new binary with all the required instrumentation already there.  
This approach nets a big advantage in the fact that, except in some rare cases, 
very low overhead is introduced, and execution speed is comparable to 
compile-time instrumentation. Furthermore, static rewriters are able to add 
complex instrumentation that is computationally expensive to introduce, as 
since it is done statically, it won't introduce unnecessary delays at runtime.

Without runtime information, to correctly perform instrumentation static 
rewriters need to rely on complex analysis, which is inherently imprecise and 
often falls back on heuristics. It turns out that the common disadvantage of 
static rewriters is that they do not scale well on bigger binaries, or binaries 
that do not follow standard patterns. In fact, virtually no static rewriter 
supports packed binaries or self-modifying code. Moreover, many rewriters 
struggle with binaries produced by deprecated compilers or with aggressive 
optimization flags. 

More recent static rewriters such as Ramblr \cite{ramblr}, Uroboros 
\cite{uroboros}, and \sysname rely on \emph{symbolization} (sometimes also 
called \emph{reassemblable assembly}), which work around the rigid structure of 
binaries by substituting hard coded references with assembly labels, that are 
more flexible and can be parsed by any generic off-the-shelf assembler.  
\sysname's approach is particularly interesting in the fact that it avoid 
heuristics to differentiate between scalars and references by focusing on 
position-independent executables (PIE). 



\section{Instrumentation details}
In this section we will go into more detail on our particular use-case of 
instrumentation, \emph{fuzzing}, and explain what is the basis of 
AdressSanitizer, the instrumentation pass we implemented in the ARM port of 
\sysname. 

\subsection{Fuzzing}
Automatic vulnerability discovery techniques are getting a lot of traction 
lately, mostly because software is getting ever more complex and large, and 
manual analysis and auditing does not scale very well. Fuzzing is certainly one 
of the most interesting automatic vulnerability discovery techniques.  It 
relies on the pseudo-random generation of test cases to give as input to a 
target binary, trying to find a specific input that makes the binary get into 
an invalid or undefined state, as it is a good indicator of a possibly 
exploitable vulnerability. This technique got even more popular after the 
release of \texttt{AFL} \cite{afl}, a fuzzer that relies on coverage  
information to generate new test cases to maximize the amount of instructions 
tested by each new input. 

Most state-of-the-art fuzzers rely on instrumentation to improve vulnerability 
discovery, as it makes the fuzzing process much more efficient. A fuzzer that 
focuses on binaries for which no instrumentation is available is commonly 
called a \emph{black-box fuzzer}. 

\subsection{ASan}
\emph{AddressSanitizer}, or ASan in short, is one of the most common static 
memory sanitization checks that can be added to a binary through a compiler 
pass, implemented in both the \texttt{clang} and \texttt{gcc} family of 
compilers. This compiler pass helps finding bugs by actively checking for 
memory corruptions, hooking calls to libc's  \texttt{free} and \texttt{malloc} 
functions. ASan is not only used by developers to debug their code before 
releasing to production, but it is also extensively used by fuzzers, as ASan 
will detect a memory violation as soon as it happens, letting the fuzzer know 
earlier and with more reliability when a bug was found. 

ASan works by introducing a new region of memory called \emph{shadow memory}, 
with a size of exactly 1/8 of the whole virtual memory available to a process.
By keeping track of each call to \texttt{malloc} and \texttt{free}, ASan stores 
in the shadow memory a compressed layout of the valid memory in the original 
virtual space, and sets up \emph{red zones} to highlight invalid or freed 
memory, that trigger a security violation as soon as they are accessed. Despite 
its non-negligible overhead (Around 73\% on average \cite{asanoverhead}), ASan 
is widely used thanks to the absence of false-positives, and for its usefulness 
in detecting memory corruption vulnerabilities which are still extremely 
popular in C/C++ codebases.




%\todo{Give some examples of instrumentation commonly used.  Get more in-depth 
%on how certain kinds of instrumentation are particularly useful for fuzzing}


%\todo{Notes from Mathias:
%One paragraph each about ASan, fuzzing, and maybe CFI. Say what each
%instrumentation looks like and reference a paper for RW.}


\section{The ARM architecture}
We will provide a short summary of what are the main differences between 
\texttt{x86} and ARM that proved to be source of non trivial problems during 
the porting of \sysname.
\begin{itemize}
	\item \emph{Fixed-size instruction set}: Contrary to \texttt{x86}, the ARM 
		instructions are all of the same size, fixed to the value of 4 bytes.  
		While this is not a problem in itself, it means that a pointer cannot 
		fit into a single instruction. To store a pointer in a register in ARM, 
		there are two main options: the first is using a region of data where 
		the pointer is hard-coded at compile time, called a \emph{literal 
		pool}; the second one is building the pointer in a multi-stage fashion 
		by using arithmetic operations.  While the first one is easier, it is 
		also less performant, and compilers will always resort to the second 
		when possible. This makes very hard recovering information about global 
		variable accesses.
	\item \emph{Jump table compression}: On \texttt{x86}, jump tables are 
		stored as list of pointers in a data section (usually 
		\texttt{.rodata}), with one pointer for each case of the jump table.  
		Instead, on ARM, jump tables are stored as offsets from the base case.  
		This is because the compiler will try to compress the jump table, and 
		in most cases a single byte is enough to indicate the offset from the 
		base case to for each case of the jump table. This leads to problems 
		for static rewriting: first of all jump tables are harder to detect, as 
		on \texttt{x86} scanning the data sections for arrays of valid 
		instruction pointers was a quite reliable way of detecting jump tables, 
		while on ARM they are indistinguishable from random bytes; secondly 
		inserting too much instrumentation between cases of the same jump table 
		could lead to the offset not fitting into a single byte anymore, and 
		breaking the whole jump table structure in memory.  Finally, extracting 
		the number of cases of a jump table is quite harder in ARM, since it is 
		impossible to scan cases until an invalid pointer is found, as like 
		stated before, jump table entries in ARM are indistinguishable from 
		random bytes.
	\item \emph{Discontinuities in immediates}: Some ARM instructions, like 
		\texttt{ADD}, support having immediates as one of the operands.  
		However, they do not accept a classical range of immediates like in 
		\texttt{x86}, but instead a specific set of values that may not be 
		continuous.  For example the \texttt{ADD} instruction can use only 
		immediates that can be expressed with a value of 8 bits scaled by a 
		\texttt{ROR} with a
		4 bits value.
	\item \emph{Not enough mature tools}: The popularity of ARM CPUs is still 
		relatively new and the tooling is not mature enough, as in fact we 
		found bugs in both the disassembler we chose to use (Capstone 
		\cite{capstone}) and the debugger (GDB)
\end{itemize}

%\todo{Get into detail on what are the key differences between x86 and ARM. Give 
%some foreshadowing on the challenges that will be faced because of ARM quirks.}



%%%%%%%%%%%%%%%%
\chapter{Design}
%%%%%%%%%%%%%%%%
%Introduce and discuss the design decisions that you made during this project.
%Highlight why individual decisions are important and/or necessary. Discuss
%how the design fits together.
%This section is usually 5-10 pages.

In this section we will go over the design decisions that we made during the 
development of the ARM version of \sysname. We will start from goals that we 
put ourselves at the start of the project, and then follow up with the key 
issues that we encountered caused by the quirks of the ARM architecture. We 
will then go over the solution we adopted for those problems both in the 
\emph{symbolization} and in the \emph{instrumentation} sides of \sysname. 

\section{Goals}


Our goal is to develop a modular platform on which it is easy to develop 
instrumentation passes for ARM binaries, with the example implementation of a 
memory sanitization pass.  We identify the following requirements that our 
solution should satisfy:

\begin{itemize}
	\item \textbf{RQ1:} The implementation should supports COTS binaries, and 
		should scale well to large binaries.
	\item \textbf{RQ2:} The symbolization and instrumentation processes must 
		not hinder the intrinsic functionality of the binary being rewritten.
	\item \textbf{RQ3:} The memory safety instrumentation pass we develop must 
		be competitive in performance and functionality with its source-based 
		counterpart.


\end{itemize}

%\todo{Give a rundown of specific goals that we set ourselves at the start of 
%the project:}
%\begin{itemize}
	%\item Study the ARM architecture and the main differences from x86
	%\item Study the codebase of the \sysname project, understand the theory 
		%behind it and its implementation
	%\item Write tests to ensure the robustness of the implementation
	%\item Implement and introduce support for ARM in the \sysname project
	%\item Develop a sample instrumentation pass (BASAN)
	%\item Evaluate the resulting implementation against state-of-the-art 
		%techniques
%\end{itemize}
	 

\section{Key Issues}
We will now go over the issues we encountered while porting \sysname to the 
ARM architecture that we briefly mentioned in the previous chapter.  

\subsection{Fixed size instruction set}
The Aarch64 instruction set is defined as \emph{fixed size}, because every 
instruction is large exactly 4 bytes. This makes the CPU design simpler,
helps keeping memory always aligned, and permits the CPU to fetch multiple 
instructions at once, since decoding is not necessary to determine instruction 
boundaries. However, despite the many advantages of this characteristic, there 
are some drawbacks too, including not being able to store a pointer in a single 
instruction (as pointers have a size of 8 bytes). The Aarch64 ISA provides two 
main solutions to this problem. 

The first one consists in storing the pointers in a special read-only region of 
memory, called a \emph{literal pool}, and then load those pointers into a 
registers using the special construct \texttt{ldr <reg>, =pointer}, a 
pseudo-instruction that the assembler will translate with the correct memory 
address once \texttt{pointer} has been stored in a literal pool.  Since all 
\emph{ldr} instructions are PC-relative, and since there are 21 bits available 
to store the offset from the PC, the assembler will store \emph{pointer} in a 
literal pool which is in the $\pm$ 1MB range of the \texttt{ldr} instruction.  
While this is a simple and straightforward approach, very useful in the case of 
hand-written assembly, this requires an additional memory access that may 
impact performance in the long run. Furthermore, the assembly will fail if it 
is not possible to store a literal pool in the given range, such as in the case 
of a function larger than 2MB. In that case, it is up to the programmer to find 
a suitable spot for the literal pool, by manually specifying its location with 
the \texttt{.ltorg} assembly directive. It is often recommended to store 
literal pools directly after non-conditional jumps to avoid executing them 
\cite{literalpools}.

The second solution is to build pointers using multiple instructions and basic 
arithmetic primitives. Aarch64 provides instructions such as \texttt{adrp 
<reg>, pointer}, which loads the base page of a pointer into a register. It is 
a PC-relative instruction , and is able to target pointers in the $\pm$ 4GB 
range. In other words, the \texttt{adrp} instruction can point only to memory 
locations that are 4KB aligned. Usually the instruction can be followed by an 
\texttt{add}, a \texttt{sub} or an offset-based load such as \texttt{ldr 
<register>, [<base\_page>, offset]}. This second way, while more contrived and 
certainly harder to read, is faster than the first one as it does not require a 
memory access, and also often benefits from custom hardware optimizations (such 
as in the Cortex A72, one of the most common ARM CPUs 
\cite{pointeroptimizations}). 

\subsubsection{The global variable problem}
For the reasons stated above, compilers generally use the second option, 
preferring performance over assembly readability. Having each pointer value 
separated in two different instructions makes the static analysis of a binary 
substantially harder. Furthermore, compiler optimizations frequently exacerbate 
the problem, by reusing parts of some pointer values to build new ones, or 
reordering instructions around in such a way that a pointer can be built on two 
instructions which are kylobytes away from each other. In some extreme cases, 
by enabling the \texttt{-O3} optimizations, we found instances of pointers 
built on two instructions that were on different functions, due to the compiler 
optimizing a macro in the C source code. 

An important factor to note is that this behaviours happens exclusively when 
accessing \emph{global variables}, since all code pointers and local variables 
are reachable through a single pc-relative instruction. 

In the symbolization process (that will be explained in detail later), we need 
to know the value of every pointer used in the program, in order to correct it 
when we will add instrumentation later on. Thus we are required to develop an 
analysis technique that lets us recover the value of every single global 
pointer used in the binary. We will now shortly describe the ideas behind the 
solution we implemented.

\subsubsection{Our solution for the global variable problem}

At first, some basic static analysis is performed on the binary, in order to 
recover functions, control flow, basic blocks and disassembly of the 
\texttt{.text} section. After this, we scan the disassembly for each possible 
instance of pointer building in the binary. After analyzing common compiler 
patterns, we found out that the \texttt{adrp} instruction (which loads the base 
page address of a pointer) is an indicator of a possible start of a pointer 
building process. 

After collecting all the possible instances of pointer building, the next step 
is to find out the final pointer value of each one. This turned out to be an 
extremely difficult task, as we soon found out that there are infinitely many 
ways of how a pointer can be built. We implemented a pattern-matching solution 
at first, trying to detect common compiler patterns for pointer building; while 
we correctly found out the value of the vast majority of pointers, a single 
mistake could make the binary crash, and our solution was not working on 
binaries of large sizes, as we inevitably failed to parse at least one or two 
edge-cases.

We later shifted to a different approach: instead of trying to find exact value 
of a pointer by pattern matching, we take the set of all possible values a 
pointer could have and exclude wrong values until possible. Over 90\% of the 
times, this approach leaves only a single value. The rest of the times, only 
two or three values remain, for which we use the old pattern-matching solution 
to understand which of those is the correct one. 

This final solution scales really well, as proved by the fact that we rewrote 
very large binaries and successfully ran them through long benchmarks. For more 
details on how the exclusion algorithm works, see the next chapter, 
Implementation.  \todo{ maybe go into detail and explain the section alignment 
trick, and what is the reason for why it works, that is retrowrite will never 
instrument data sections}


\subsection{Jump Tables}
There is a big difference in how jump tables in ARM are implemented compared to 
\texttt{x86}. In fact, in \texttt{x86}, a jump table is represented through a 
list of code pointers in the \texttt{.rodata} section. The assembly generated 
by the compiler will simply load the pointer from the list indexed by the 
number of the case that is going to be executed. 

On ARM, things are different: jump tables are stored as a list of 
\emph{offsets} from the base case (case number 0) in memory. The compiler 
generates assembly that fetches the correct offset based on the case number 
from the list in memory, adds the offset to the base case, and jumps to the 
resulting value. Listing \ref{lst:jmptbl} shows an example of jump table access 
in \texttt{aarch64}. The first two instructions build a global pointer to where 
the jump table is stored in memory. In line 3 the offset to the corresponding 
case is loaded into register \texttt{w1}, and then later added to the base case 
\texttt{x0} on line 5. 


\begin{lstlisting}[label={lst:jmptbl},caption={Example of a jump table in \texttt{aarch64}}]
adrp x0, <jump_table_page_address>
add x0, x0, <jump_table_page_offset>
ldrb w1, [x0, w1, uxtw]
adr x0, <base case address>
add x0, x0, w1, sxtb 2
br x0
\end{lstlisting}

Furthermore, jump tables in \texttt{aarch64} are complicated by the fact that 
they are often \emph{compressed} in memory. Since they store offsets, not 
pointers, and commonly jump table cases are very close to the base case, 
compilers usually avoid using the full 8 bytes of memory for each case (which 
would be normal in \texttt{x86}), but will use less if possible. For instance, 
if all offsets are less than 256, the compiler will use a single byte in memory 
to store each case.  

\subsubsection{Detection of jump tables}
The first problem we had to face was the discovery of jump tables. While they 
have a very distinct pattern (a load from memory, followed by some arithmetic, 
and then an indirect jump), many other constructs share similar patterns (like 
using a callback in a struct). We found out that a reliable way of detecting 
them is by backward-slicing every time the disassembler encountered an indirect 
jump, and then verifying if the value of the register used for the indirect 
jump could be represented with an expression which could be resolved statically 
and matched a very defined pattern (load 1/2/4 bytes from memory, load a base 
address, add the offset and then jump to the result). 

To represent the value of a register as an expression, we developed a simple
pseudo-emulation engine that steps backwards from a given instruction, 
following control flow and building step by step the resulting expression, 
similar to what a dumbed-down symbolic executor would output. The 
pseudo-emulation engine is limited, supports circa 20 instructions, as 
emulating ARM was out of the scope of the project and we only needed it for 
jump table detection. A detailed explanation of how it works can be found in 
the next chapter.

\subsubsection{Detection of jump tables size}
Another problem that quickly arose from the peculiarities of ARM jump tables is 
that it is much harder to estimate the number of cases that a jump table 
supports, compared to \texttt{x86}. In fact, in \texttt{x86}, simple heuristics 
such as scanning memory for contiguous sections of valid instruction pointers 
until an invalid one is found can go a long way. However, as stated before, in 
ARM jump tables are indistinguishable from random bytes, so it is impossible to 
use heuristics to understand the bounds of a particular jump table in memory.

We found that backward slicing is again a robust solution here too. After 
detecting a jump table, we can identify the instruction that takes care of 
loading the offset from memory, and from there we mark the register that holds 
the value of the number of the case that is going to be executed.  
Backward-slicing until a comparison operation is performed on the marked 
register, bounding the number of cases to an absolute number, turned out to be 
a very reliable solution.  

\subsubsection{Jump table offset overflow}
Another problem spawned from how jump tables are represented in ARM comes up 
when instrumenting a function that contains a jump table. In fact, it is very 
likely that adding too much instrumentation inside a single case could 
\emph{overflow} one of the offsets that stored its distance from the base case.
Especially when maximum compression is used and offsets are stored in a single 
byte, it is very common to overflow multiple of them even with light 
instrumentation. 

This was one of the harder problems to fix, and we considered the following 
solutions:
\begin{itemize}
	\item Expand the jump table in memory. Enlarge the \texttt{.rodata} section 
		and move everything to make space in memory for the expanded jump 
		table. While possible, this would have been be a drastic change that 
		was not scalable or easily implemented.
	\item Create a new jump table in a new section, and patch the pointer 
		building code at the start of the jump table access code. While this 
		was the easiest solution to implement, we discarded it because of the 
		additional space required and its poor scalability.
	\item Divide all the offsets by the same constant value. Normally, all 
		offsets of a jump table represent the distance between a case and the 
		base case expressed in bytes divided by 4. This is because each 
		instruction is 4 bytes long, and it would not make sense to point 
		inside an instruction. In fact, in Listing \ref{lst:jmptbl}, line 5, we 
		can see how the offset is shifted by 2 to the left (so multiplied by 
		4). This is automatically done by the compiler. However, we can use the 
		same technique the compiler uses and store offsets divided by 8, 16 or 
		more, and changing how much the offset is shifted to the left before 
		being used, thus enabling us to store larger differences in a single 
		byte. 

		The trade-off with this approach is that offsets can no longer point to 
		a single instruction, but to a block of 2, 4 or more instructions, 
		depending on how much enlargement was needed. To make sure that each 
		offset points to the right instruction, some light \emph{nop-padding} 
		is applied between cases to make sure that alignment is correct every 
		time.
\end{itemize}

We ended up using the last solution, as even if it was slightly more complex to 
implement, it would help us keep the original memory layout of the binary, 
which is a very desirable property in binary rewriting. 

\subsection{Control Flow broken by instrumentation}
\todo{not sure where to put this subsection. Maybe delete it, it's a really 
simple issue, maybe not worth discussing}
When adding substantial amount of instrumentation to a binary, some pc-relative 
branches can break, like the instruction \texttt{cbz}, which is able to jump to 
addresses not farther than 1MB. 

In this cases we fix the relevant instruction by making them point to a some 
additional instrumentation containing a trampoline to the original target of 
the branch. 


\section{Symbolizer}
\todo{Add a figure for the system overview here, with a short explanation on 
how it is split into a single Symbolizer and multiple possible instrumentation 
modules}

\todo{Not really sure what to write here, as the ideas behind the symbolization 
process are basically the same to x86, nothing new/fancy here.}




\section{Instrumentation (BASan)}
The ASan instrumentation was designed to be compatible with the 
AddressSanitizer libraries provided by Google, \texttt{libasan}. We carefully 
selected shadow memory offsets and sizes to match those included in 
\texttt{libasan}.  The library will hook on each call to \texttt{malloc} and 
\texttt{free}, writing in the shadow memory the available bytes that can be 
used. \sysname takes care of finding each access in memory and inserting 
instrumentation just before each access to check the relevant bytes of shadow 
memory and error out in case an overflow or other memory corruption was found.


\begin{lstlisting}[language=C,label={lst:asan},caption={ASan checking algorithm 
implemented in C}]
byte shadow_value = *(MemToShadow(address));
if (shadow_value) {
  if ((address & 7) + AccessSize - 1 >= shadow_value) {
	ReportError(address, AccessSize);
  }
}
\end{lstlisting}

Listing \ref{lst:asan} shows the ASan checking algorithm in high level. To 
implement it as an instrumentation pass, we manually wrote assembly code that 
matched its functionality and could be adapted to both reads and stores.  
Different versions of ASan snippets were developed depending on the size of the 
store/load, to ensure maximum efficiency and avoid wasting calculations at 
runtime. An example of an instrumented instruction can be found in the listing 
below:

\todo{Add comparison between non-instrumented and instrumented 8-byte load (the 
simplest case)}

Unfortunately, the BASan instrumentation pass is not completely equivalent to 
its source based counterpart, in fact we can highlight the following 
differences:
\begin{itemize}
	\item Missing global variable bounds checking: without the source code, it 
		is impossible to distinguish boundaries between global variables in 
		data section. For this reason, checks on global variables are disabled.
	\item Missing stack variables bounds checking: similar to above, without 
		source code it is extremely hard to find boundaries between variables 
		on the stack.
	\item Number of instrumented locations: ASan used as a compiler pass is 
		able to prune the checking on a lot of memory accesses, since on many 
		instances the safeness of a memory access can be determined at 
		compile-time with the source code at hand. However, \sysname works only 
		on binaries and thus is forced to instrument all memory accesses.
\end{itemize}
Despite the following limitations, the BASan instrumentation pass is able to 
provide the same core functionality of the original ASan compiler pass (memory 
sanitization on the heap), and with very little additional overhead.

\subsubsection{Optimization: register savings}.
The snippets of assembly used to implement ASan memory checking require some 
temporary register usage to store intermediate calculations, and thus require 
special precautions before inserting them in the middle of a binary. Our first 
approach was to save the values of each register we were planning to use on the 
stack, and then restore the old values as the last step of the instrumentation.  
However, we quickly realized that those frequent memory accesses were 
introducing noticeable overhead. 

We then switched to performing a static analysis of register usage on every 
function of the binary, in such a way that at any given instruction we know 
which registers can be freely used and which cannot be modified without 
hindering the correct execution of the binary. Then, when inserting the 
instrumentation, we modify the snippets in such a way that they will try to use 
as many "free" registers as possible, while saving the others on the stack.

Of particular note is the saving of the value of register \texttt{ncvz}, which 
holds the condition flags in \texttt{aarch64}. This register is always saved 
and restored at each instrumented location, as all ASan snippets contain a 
conditional branch for the error checking. 

\todo{If needed, put other optimizations here, like batching}



%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Implementation}
%%%%%%%%%%%%%%%%%%%%%%%%
%The implementation covers some of the implementation details of your project.
%This is not intended to be a low level description of every line of code that
%you wrote but covers the implementation aspects of the projects.

%This section is usually 3-5 pages.


\section{Symbolizer}
\todo{Go into the nitty-gritty details of my horrible hackish implementation, }

\subsection{Global variables}
\todo{}
\subsubsection{How were they detected}
\subsubsection{How were they fixed}
\subsubsection{The literal pool dilemma}

\subsection{Jump Tables}
\subsubsection{How were they detected}
\todo{Pseudo-emulating ARM binaries to detect jump tables}
\subsubsection{How were they fixed}
%\todo{Explain the following:}
%\begin{itemize}
	%\item List all ideas we had
	%\item Explain why we chose to implement the ungodly hack of expanding them 
	%\item also, nop padding between cases
%\end{itemize}

\section{ASan instrumentation}
\todo{Overview of the ASan algorithm, how it was hand-translated to ARM 
assembly}
\todo{Various optimizations that were introduced to lower instrumentation 
overhead}


%%%%%%%%%%%%%%%%%%%%
\chapter{Evaluation}
%%%%%%%%%%%%%%%%%%%%
%In the evaluation you convince the reader that your design works as intended.
%Describe the evaluation setup, the designed experiments, and how the
%experiments showcase the individual points you want to prove.

%This section is usually 5-10 pages.

\section{Experiment overview}
\todo{Explain our setup, the hardware we used, what we were measuring.}

\todo{Talk a bit on also the difficulties caused by the nature of the 
benchmarks, namely very long times (up to 48 hours, and probably more now that 
I have CloudLab :D ), very high RAM usage, and so on.}

\todo{(OPTIONAL) Talk a bit on how those difficulties were approached, with the 
very strict collection of experiment metadata using Github CI and the telegram 
bot.}

\section{SPEC CPU}
\todo{Explain how much do I hate the SPEC CPU benchmark, who developed it, the 
crazy fact that it is the accepted state of the art method for benchmarking 
single-core performance, how counter-intuitive their directory structure, 
config files, and bash scripts are.  /s}

\section{Results}
\todo{Present  plots and tables, and a detailed explanation on each one.
Make sure to respect the style of Mathias' previous papers/theses, to avoid 
getting him unnecessarily angry}

\todo{Compare it to state-of-the-art dynamic instrumentation (QEMU).}


%%%%%%%%%%%%%%%%%%%%%%
\chapter{Related Work}
%%%%%%%%%%%%%%%%%%%%%%

%The related work section covers closely related work. Here you can highlight
%the related work, how it solved the problem, and why it solved a different
%problem. Do not play down the importance of related work, all of these
%systems have been published and evaluated! Say what is different and how
%you overcome some of the weaknesses of related work by discussing the 
%trade-offs. Stay positive!
%This section is usually 3-5 pages.


% useful links:
% REPICA: https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8454360
%         good state of the art overview, both ARM64 and arm32
%         1. do not use symbolization, but relative address correction
%         2. Detect jump tables and global pointers, but not fix them
%         3. Overhead is exactly the same as retrowrite
% A SURVEY ON BINARY REWRITING: https://publications.sba-research.org/publications/201906%20-%20GMerzdovnik%20-%20From%20hack%20to%20elaborate%20technique.pdf

{

\setlength{\parindent}{0cm}

\todo{This section now has only list of related work I would like to talk 
about, will be expanded later}



\textbf{Dynamic rewriters in general}:\\
First, the good ol' ones: \texttt{PIN}\cite{pin}, 
\texttt{Valgrind}\cite{valgrind} and \texttt{DynInst}\cite{dyninst}.

More recently, \texttt{Multiverse} \cite{multiverse}, \texttt{Frida} and 
\texttt{DynamorIO} are interesting examples.

\texttt{QASAN}\cite{qasan} is similar to \sysname's BAsan, much slower but 
also much more portable (works on any binary supported by QEMU)



\textbf{Static rewriters that use lifting to IR}:\\
\texttt{McSema} \cite{mcsema} is a great example of LLVM IR lifting approach, 
also particularly nice as it supports C++ exceptions

\textbf{Static rewriters that use trampolines}:\\
\texttt{E9patch}\cite{e9patch} uses only trampolines, no need to recover 
control flow, but also noticeable overhead

%\todo{Also say that trampolines are especially effective on ARM because every 
%instruction is always 4 bytes, so you don't need to worry about overwriting 
%instructions shorter than a jmp like in x86}

\textbf{Static rewriters that use symbolization}:\\
\texttt{Uroboros}\cite{uroboros} and \texttt{Ramblr}\cite{ramblr} are the first 
reassemblable assembly approaches (symbolization), \sysname 
\cite{dinesh20oakland} (x86 version)

\textbf{Static rewriters aimed at ARM binaries}:\\
\texttt{Repica} \cite{repica} is probably the most recent addition to binary 
rewriters aimed at ARM binaries.  

\todo{Maybe add Bistro ? } \cite{bistro}

For more check out recent surveys on the area of binary rewriting 
\cite{binaryrewritingsurvey}

}

%%%%%%%%%%%%%%%%%%%%%
\chapter{Future Work}
%%%%%%%%%%%%%%%%%%%%%

{

\setlength{\parindent}{0cm}
\hangindent=0.7cm \textbf{Support for more source languages}: For now, 
\sysname supports only binaries compiled from the C language, both for the 
\texttt{x86} and the ARM implementation. The easiest addition would be to add 
support for C++ by expanding the analysis capabilities of \sysname to support 
exception tables too, but many more languages could be supported in the future. 

\hangindent=0.7cm \textbf{Support for kernel space binaries}: Right now the ARM 
port of \sysname supports only userspace binaries, contrary to the 
\texttt{x86} version that supports linux kernel modules too. The kernel version 
of \sysname\_ARM would prove to be particularly interesting as it would open 
new ways to efficiently fuzz Android kernel modules.

\hangindent=0.7cm \textbf{Support for more executable formats/operating 
systems}: The current implementation of the \sysname tool is aimed only 
towards ELF files, but adding support for MACH-O and PE binaries should not 
require too much effort. This would also be interesting as Windows and MacOS 
present way more closed-source modules compared to Linux.

\hangindent=0.7cm \textbf{More instrumentation passes}: While right now we 
implemented only the AddressSanitizer instrumentation in the ARM port of
\sysname, the design of \sysname is modular and adding new instrumentation 
passes or new mitigations should be easy. To name a few, the interesting ones would be:
\begin{itemize}
	\item Shadow stack (return address protection)
	\item Control flow authentication using ARM pointer authentication 
		(hardware-assisted)
	\item Coverage-guidance for fuzzing
\end{itemize}

}

%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}
%%%%%%%%%%%%%%%%%%%%

%In the conclusion you repeat the main result and finalize the discussion of
%your project. Mention the core results and why as well as how your system
%advances the status quo.


In summary, we develop the ARM architecture implementation of the \sysname 
project, a scalable static rewriter for linux C binaries.  \sysname enables 
targeted application of static instrumentation where no source code is 
available, such as proprietary binaries, inline assembly, or code generated by 
a deprecated compiler.  We also present an example instrumentation pass on top 
of the symbolization engine, AddressSanitizer, particularly useful for fuzzing 
purposes. We present new solutions to problems arising from the peculiarities 
of the ARM architecture such as the fixed-size instruction set, global variable 
accesses and jump table instrumentation. We show that the total overhead of the 
symbolization and the instrumentation pass are competitive with source based 
AddressSanitizer.  Our work shows that \sysname's original approach is not 
limited to the \texttt{x86} architecture, but can be applied to the ARM 
architecture and more.



\cleardoublepage
\phantomsection

\addcontentsline{toc}{chapter}{Bibliography}
\printbibliography

% Appendices are optional
% \appendix
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{How to make a transmogrifier}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% In case you ever need an (optional) appendix.
%

\end{document}
