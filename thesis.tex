%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% EPFL report package, main thesis file Goal: provide formatting for theses and
% project reports Author: Mathias Payer <mathias.payer@epfl.ch>
%
% This work may be distributed and/or modified under the conditions of the
% LaTeX Project Public License, either version 1.3 of this license or (at your
% option) any later version.  The latest version of this license is in
% http://www.latex-project.org/lppl.txt
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% useful links:
% REPICA: https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8454360
%         good state of the art overview, both ARM64 and arm32
%         1. do not use symbolization, but relative address correction
%         2. Detect jump tables and global pointers, but not fix them
%         3. Overhead is exactly the same as retrowrite


%\pdfcompresslevel=0 \pdfobjcompresslevel=0

\documentclass[a4paper,11pt,oneside]{report}
\usepackage[MScThesis,lablogo]{EPFLreport} \usepackage{xspace}
\usepackage{xcolor} 
\usepackage{soul}
\usepackage{hyperref} 


\title{Arm Wrestling: porting the Retrowrite project to the ARM architecture}
\author{Luca Di Bartolomeo} \adviser{Prof. Mathias Payer (EPFL)}
\supervisor{Prof. Kenny Paterson (ETH)}

\linespread{1.5} 
\newcommand{\sysname}{Retrowrite\xspace}

%\definecolor{orange}{rgb}{1,0.5,0}
%\newcommand{\todo}[1]{\colorbox{cyan}{\parbox{0.9\textwidth}{#1}}}
\newcommand{\todo}[1]{%
	\begingroup 
	\sethlcolor{cyan}%
	\hl{TODO: #1}%
	\endgroup
}


\dedication{ 
\begin{raggedleft}
	No matter where you go, everyone is connected.\\
	--- Serial Experiments Lain\\
\end{raggedleft} 
\vspace{4cm} 
\begin{center}
	Dedicated to my parents, my sister Sara, my dear Giulia, to my friends back 
	in Rome and to my roommates Matteo and Filippo who all inspired me and kept 
	up with my constant complaining. Thanks!
\end{center} 
}

\acknowledgments{
	I would like to thank my advisor, Prof. Mathias Payer, for his support,
	guidance, and for trusting me by assigning me this inspiring project.  I
	admire him a lot and I wish all the best for him, and in particular I hope
	that I can work on many other projects with him.
	
	I would also like to thank his research group, HexHive, as I always found
	myself very welcome there, even if I could visit them only once a week.  
	Those have been six very happy months in which I learned quite a lot of 
	things, and I have to thank prof. Prof. Payer and his doctorate students 
	and researches for it. I wish them all to have a very succesfull career, 
	and I hope we can continue to work together in the future!
	
	Special thanks goes to my family and my friends in Rome. Their support was
	always available, and it has always been a huge pleasure to visit them once
	in a while in Italy. I also need to thank my S.O. Giulia, I felt she was
	always behind my back, keeping a good check on my mental sanity during the
	worst times of the outbreak. My roommates too, Matteo and Filippo, deserve
	a mention here, as their patience and their rubber duck debugging skills
	proved to be fundamental during some nasty debugging sessions.

	Finally, I would also like to mention my CTF team, Flagbot, that made me 
	spend so many weekends without going out but ultimately lead me to meet so 
	many new interesting people, and the teams I had played with occasionally, 
	namely polyglots and TRX. Thanks!

}

\begin{document}
\maketitle
\makededication
\makeacks


\begin{abstract}

	While there were good recent attempts, (\todo{link to PinePhone, System76,
	and similar projects}) using only open-source software is particularly
	hard, and especially on the mobile phone market even the most determined
	users are often forced to use closed-source ARM libraries or modules.  
	Those often run at privileges higher than we might want (e.g. manufacturer
	specific kernel modules, \todo{link to some}), and are also hard to audit
	for vulnerabilities. 

	Many existing tools were developed to improve the auditability of closed
	source programs, especially aimed at helping the fuzzing process, with
	approaches such as implementing AddressSanitizer (a compiler pass only
	available with the source code) through dynamic instrumentation. However,  
	even state-of-the-art dynamic instrumentation engines incur in prohibitive 
	runtime overhead (10x and more). 

	In this thesis, we would like to show that symbolization for ARM binaries 
	is a viable alternative to existing approaches, that has less flexibility 
	(only works on C, position independent binaries) but has
	negligible overhead compared to compiled source code. We present the ARM
	port of \texttt{Retrowrite}, an existing static binary rewriter for x86
	executables, which implements the symbolization engine and the memory
	sanitization instrumentation. 

	While \texttt{Retrowrite} is not the only binary rewriter for x86, there 
	are very few that are aimed at the ARM64 architecture. In particular, most 
	of them resort to lifting to an intermediate IR (achieving more flexibility 
	but losing fine-grained control over the instrumented instructions) or use 
	approaches like trampolines which make the whole problem much easier but 
	also introduce a noticeable overhead.
	\todo{add a conclusion to this abstract, like "our proof of work 
	implementation of retrowrite is xx\% fastern than QEMU and etc..."}


\end{abstract}


\maketoc

%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
%%%%%%%%%%%%%%%%%%%%%%
%The introduction is a longer writeup that gently eases the reader into your
%thesis~\cite{dinesh20oakland}. Use the first paragraph to discuss the setting.
%In the second paragraph you can introduce the main challenge that you see.
%The third paragraph lists why related work is insufficient.
%The fourth and fifth paragraphs discuss your approach and why it is needed.
%The sixth paragraph will introduce your thesis statement. Think how you can
%distill the essence of your thesis into a single sentence.
%The seventh paragraph will highlight some of your results
%The eights paragraph discusses your core contribution.
%This section is usually 3-5 pages.

% First paragraph: the setting
Security researchers routinely find vulnerabilities in major desktop OSes, but 
recently a lot of focus has shifted on the analysis of mobile operating 
systems, including Android.  More than 40 CVEs for Android were published just 
in December of 2019 
\footnote{\url{https://source.android.com/security/bulletin/2019-12-01}}.  This 
big focus on the analysis of Android has certainly a lot to do on with the rise 
of the mobile phone market in the latest years, but also with the fact that the 
source code of Android is (mostly) open and available for everyone to dig 
through and debug.

Still, digging through the code is not the only way of auditing a codebase, in 
fact a very effective technique as of today is the process of \emph{fuzzing}.  
It is taking a lot of traction because setting up a fuzzing campaign, even a 
large scale one, is relatively easy for beginners too. The main advantages of 
fuzzing compared to other automatic vulnerability discovery techniques is that 
it is not need to have a deep knowledge of the fuzzer engine to have a 
successful run, it is only required to select the initial input seed corpus, 
and implement a fuzzing harness. Moreover, the approach is extremely easy to 
parallelize, so it is not uncommon for companies to setup large server farms 
with thousands of CPU cores dedicated to fuzzing.

Having the source code available while fuzzing immensely helps the fuzzer 
engine thanks to coverage information, with which the fuzzer can prune inputs 
much more accurately compared to \emph{black-box} fuzzing without the source 
code.  Furthermore, the presence of source code gives the opportunity to apply 
other instrumentation passes, which can help the fuzzer even more.  For 
instance, memory sanitization helps the fuzzer find memory corruption bugs as 
soon as they are introduced without needing to wait for a crash. 

Unfortunately, in spite of the increasing popularity of open source software, 
closed source modules and binaries are still much more common: on desktop, 90\% 
of the operating systems are closed source, and even who runs an open-source 
alternative usually still has many closed source components (e.g., Skype, 
Nvidia drivers, various DRM components in browsers, etc.). This holds true even 
in the phone market: while 75\% \cite{statista} of mobile users use Android or 
other open-source operating systems, virtually all of those devices have 
proprietary components already installed at the moment of purchase. Most 
commonly there are Google proprietary libraries like the Play API, manufacturer 
custom modules and firmwares (for hardware peripherals such as Bluetooth or 
WWAN connectivity), and much more. In many instances, those closed-source 
modules are installed as kernel modules or run with root privileges. To make 
things worse, they are often are exposed to the network, creating opportunities 
for malware or other bad actors to take control of mobile devices.

When considering about fuzzing closed source binaries, the main difficulties 
that arise are due to the fact that the compilation process discards a large 
amount of information that could be very useful while hunting bugs, such as 
type information, control flow, array boundaries, scalars/references 
distinction, and much more.  Still, there are techniques that can help 
instrument a binary and fuzz it in those conditions, and they all fall into a 
common category called \emph{Binary rewriting}.

Binary rewriting techniques can be divided into two main approaches: 
\emph{dynamic} rewriting and \emph{static} rewriting. Dynamic rewriting 
involves running the target binary in a controlled execution alongside the 
rewriter engine, which instruments the target binary on the fly. The advantage 
of this approach is that at runtime the rewriter is able to recover some of the 
information that was lost during compilation (the most important one being the 
distinction between scalars and references), and so do not need to rely on 
heuristics, and can support a very wide variety of binaries (even packed or 
self-modifying binaries). However, the trade-off is the overhead introduced in 
the process: dynamic rewriters usually have between 10x and 100x of overhead 
\cite{dinesh20oakland}, which has a great impact on fuzzing performance. 

On the other hand, static rewriting is the opposite approach: instrument the 
target binary ahead of time, using various techniques to recover as much 
information as possible from the code (including heuristics). Flexibility of 
static rewriters is more limited, as virtually none of them support edge cases 
such as self-modifying binaries, code that does not support standard control 
flow practices (like OCaml or Haskell binaries), or even code generated by a 
non-popular compiler. However, the overhead introduced by static rewriting has 
generally low overhead, and moreover it can support more complex 
instrumentation passes (dynamic rewriters need to keep it simple for 
performance constraints). 

Since static rewriters have much less flexibility, the aim of this thesis is to 
expand the domain of supported binaries of one of the recently published static 
rewriters, Retrowrite, by Dinesh et al. \cite{dinesh20oakland}. Retrowrite uses 
the relatively new approach of \emph{symbolization}, that produces 
reassemblable assembly on which instrumentation passes may be applied to.  
Retrowrite focuses on position-independent binaries to eliminate the problem of 
distinguishing between scalars and pointers, and manages to produce 
reassemblable assembly without the usage of heuristics. 

Retrowrite supports only x86\_64 binaries in both user and kernel space; 
however, most mobile devices are running on an ARM CPU, and also some new 
desktop devices like the latest MacBook are switching to ARM. In this thesis we 
will explain our efforts to add support for userland ARM64 binaries, with the 
goal that one day we will be able to fuzz Android modules and closed source 
components. Furthermore, we will also explain how we ported to ARM the original 
instrumentation pass that the authors of Retrowrite implemented, 
\emph{AddressSanitizer}, as a proof of concept to show the usefulness of the 
symbolization approach. To the best of our knowledge, there were no previous 
attempts at statically introducing memory sanitization to ARM binaries in the 
past. 

We will show through benchmarks how the overhead of Retrowrite's 
instrumentation is very low when compared to source based instrumentation, and 
much lower than state-of-the-art dynamic instrumentation. In our experiments, 
we find that we are \todo{xx\%} slower than source based instrumentation, and 
\todo{xx\%} faster than state-of-the-art dynamic instrumentation.

Our core contributions consist in solving many challenges that static rewriting 
ARM binaries present, mostly spawning from the fact that ARM is a fixed size 
instruction set, in which pointers need to be built in multiple instructions, 
or located in memory in a special region called a literal pool. A series of new 
problems like jump table size fixing arise from this, and in fact the number of 
static rewriters for ARM binaries is very limited, and, to the best of our 
knowledge, some of those problems were never analyzed before.  We present our 
approach to fix or workaround most of those problems, and will give pointers to 
a few alternative solutions we analyzed.

We then evaluate our ARM port of Retrowrite , discuss its limitations, discuss 
related work, and present our conclusions.




% Second paragraph: main challenge

% Third paragraph: related work is insufficient, and why

% Fourth+Fifth paragraph: our approach, why is it needed

% Sixth paragraph: thesis statement (single sentence?)

% Seventh paragraph: highlight of results

% Eight paragraph: our core contribution

%%%%%%%%%%%%%%%%%%%%
\chapter{Background}
%%%%%%%%%%%%%%%%%%%%
%The background section introduces the necessary background to understand your
%work. This is not necessarily related work but technologies and dependencies
%that must be resolved to understand your design and implementation.
%This section is usually 3-5 pages.

Here we will provide a summary of the basic notions required to understand the 
underlying concepts behind Retrowrite and the problems we faced while porting 
it to the ARM architecture. 

\section{Binary Rewriting}
\emph{Binary rewriting} describes the alteration of a compiled program without 
having the source code at hand in such a way that the binary under 
investigation stays executable. The applications of binary rewriting are 
multiple and can be summarized as follows:
\begin{itemize}
	\item \textbf{Emulation}: An emulator is a special software that mimics the 
		behaviour of a device while executing on a different device. Emulators 
		use binary rewriting to translate system calls, instructions, memory 
		access and all the other execution primitives from one processor 
		architecture to another.  An example of this would be QEMU \cite{qemu}

	\item \textbf{Optimization}: In high performance computing domains, having 
		a way to patch subtle things like cache misses or timing anomalies in 
		very long running tasks without the need to restart the whole program 
		is of special interest. In such situations, binary rewriting is a 
		solution for run-time patching, as it is done by DynInst \cite{dyninst} 
		or Frida \footnote{\url{https://frida.re/}}

	\item \textbf{Observation}: Having an in-depth look during the execution of 
		a binary by inserting profiling or tracing instructions in the middle 
		of its code can prove to be particularly useful in many applications, 
		like catching memory leaks (e.g., Valgrind \cite{valgrind}), coverage 
		information for fuzzing (e.g., AFL-QEMU \cite{afl}) and more.

	\item \textbf{Hardening}: The absence of source code, combined with no 
		vendor support, or deprecated build tools, could make the use of binary 
		rewriting necessary to introduce security measures such as the 
		insertion of stack canaries (e.g., Stackguard \cite{stackguard}) or 
		memory sanitization (e.g., Retrowrite \cite{dinesh20oakland}).
	
\end{itemize}

Instrumenting a binary is not an easy task, as it is often said that binaries 
have a \emph{rigid} structure, because moving any data or code, or inserting 
instructions, will break all the references inside said binary. Thus, a binary 
rewriter must make sure that all the pointers and control flow broken by the 
inserted instrumentation must be corrected.

In the next section we will analyze the difference between the two different 
approaches that can be taken for binary rewriting, namely \emph{dynamic} and 
\emph{static} instrumentation.




\section{Dynamic and Static Instrumentation}

\subsection{Dynamic instrumentation}
Dynamic rewriters modify and instrument the code of the target binary during 
runtime. Usually, the target binary is executed in a controlled environment 
side by side with the rewriter engine, which patches instructions and fixes 
references on the go. Sometimes the rewriter engine leverages the operating 
system's primitives to control the execution of the target, like using the 
\texttt{ptrace} system call on Linux, but there are notable cases in which the 
rewriter engine comes with their own instrumentation runtime (e.g., Dynamo 
\cite{dynamo}) or implement a full featured virtual machine (e.g., STRATA 
\cite{strata}).

The big advantage of dynamic rewriters is the additional information that is 
only available at run time, like the path that the execution has taken.  
Furthermore, dynamic rewriters can avoid analyzing the whole binary at once, as 
they can just focus on the part that is being currently executed, making them 
scalable to arbitrarily large programs.

However, the additional runtime information comes at a high performance cost: 
running the rewriter engine alongside the target binary is expensive, and 
furthermore the frequent context switches the CPU must do to execute both make 
the situation even worse. The total overhead for an instrumentation pass like 
memory sanitization for a state-of-the-art dynamic rewriter like Valgrind are 
in the order of 10x \cite{dinesh20oakland} the overhead introduced by 
source-level memory sanitization .

\subsection{Static instrumentation}
Static rewriters process the target binary \emph{before} execution, and produce 
as output a new binary with all the required instrumentation already there.  
This approach nets a big advantage in the fact that, except in some rare cases, 
very low overhead is introduced, and execution speed comparable to compile-time 
instrumentation. Furthermore, static rewriters are able to add complex 
instrumentation that is computationally expensive to introduce, as since it is 
done statically, it won't introduce unnecessary overhead in the target binary. 

Without runtime information, static rewriters need to rely on complex analysis 
to instrument, which are inherently imprecise, and sometime fall back on 
heuristics. It turns out that the common disadvantage of static rewriters is 
that they do not scale well on bigger binaries, or binaries that do not follow 
standard patterns. In fact, virtually no static rewriter supports packed 
binaries or self-modifying code. Moreover, many rewriters struggle with 
binaries produced by deprecated compilers or with aggressive optimization 
flags. 

More recent static rewriters such as \texttt{Ramblr} \cite{ramblr}, 
\texttt{Uroboros} \cite{uroboros}, and \texttt{Retrowrite} rely on 
\emph{symbolization} (sometimes also called \emph{reassemblable assembly}), 
which work around the rigid structure of binaries by substituting hard coded 
references with assembly labels, that are more flexible and can be parsed by 
any generic off-the-shelf assembler. \texttt{Retrowrite}'s approach is 
particularly interesting in the fact that it avoid heuristics to differentiate 
between scalars and references by focusing on position-independent executables 
(PIE). 



\section{Instrumentation details}
In this section we will go into more detail on our particular use-case of 
instrumentation, \emph{fuzzing}, and explain what is the basis of 
AdressSanitizer, the instrumentation pass we implemented in the ARM port of 
Retrowrite. 

\subsection{Fuzzing}
Automatic vulnerability discovery techniques are getting a lot of traction 
lately, mostly because software is getting ever more complex and large, and 
manual analysis and auditing does not scale very well. Fuzzing is certainly one 
of the most interesting automatic vulnerability discovery techniques.  It 
relies on the pseudo-random generation of test cases to give as input to a 
target binary, trying to find a specific input that makes the binary get into 
an invalid or undefined state, as it is a good indicator of a possibly 
exploitable vulnerability. This technique got even more popular after the 
release of \texttt{AFL} \cite{afl}, a fuzzer that relies on coverage  
information to generate new test cases to maximize the amount of instructions 
tested by each new input. 

Most state-of-the-art fuzzers rely on instrumentation to improve vulnerability 
discovery, as it's a much more efficient to do so. When no instrumentation is 
available on a target binary, we usually use the term \emph{Black-box fuzzing}. 

\subsection{ASan}
\emph{AddressSanitizer}, or ASan in short, is one of the most common static 
memory sanitization checks that can be added to a binary through a compiler 
pass, implemented in both the \texttt{clang} and \texttt{gcc} family of 
compilers. This compiler pass helps finding bugs by actively checking for 
memory corruptions, hooking calls to libc's  \texttt{free} and \texttt{malloc} 
functions. ASan is not only used by developers to debug their code before 
releasing to production, but it is also extensively used by fuzzers, as ASan 
will detect a memory violation as soon as it happens, letting the fuzzer know 
faster and with more reliability when a bug was found. 

ASan works by introducing a new region of memory called \emph{shadow memory}, 
which a size of exactly 1/8 of the whole virtual memory available to a process.
By keeping track of each call to \texttt{malloc} and \texttt{free}, ASan stores 
in the shadow memory a compressed layout of the valid memory in the original 
virtual space, and sets up \emph{red zones} to highlight invalid or freed 
memory, that trigger a security violation as soon as they are accessed. Despite 
its non-negligible overhead (Around 73\% on average \cite{asanoverhead}), ASan 
is widely used thanks to the absence of false-positives, and for its usefulness 
in detecting memory corruption vulnerabilities which are still extremely 
popular in C/C++ codebases.




%\todo{Give some examples of instrumentation commonly used.  Get more in-depth 
%on how certain kinds of instrumentation are particularly useful for fuzzing}


%\todo{Notes from Mathias:
%One paragraph each about ASan, fuzzing, and maybe CFI. Say what each
%instrumentation looks like and reference a paper for RW.}


\section{The ARM architecture}
We will provide a short summary of what are the main differences between x86 
and ARM that proved to be source of non trivial problems during the porting of 
Retrowrite.
\begin{itemize}
	\item \emph{Fixed-size instruction set}: Contrary to x86, the ARM 
		instructions are all of the same size, fixed to the value of 4 bytes.  
		While this is not a problem in itself, it means that a pointer cannot 
		fit into a single instruction. To store a pointer in a register in ARM, 
		there are two main options: the first is using a region of data where 
		the pointer is hard-coded at compile time, called a \emph{literal 
		pool}; the second one is building the pointer in a multi-stage fashion 
		by using arithmetic operations.  While the first one is easier, it is 
		also less performant, and compilers will always resort to the second 
		when possible. This makes very hard recovering information about global 
		variable accesses and callbacks.
	\item \emph{Jump table compression}: On x86, jump tables are stored as list 
		of pointers in a data section (usually \texttt{.rodata}), with one 
		pointer for each case of the jump table. Instead, on ARM, jump tables 
		are stored as \emph{offsets from the base case}. This is because the 
		compiler will try to compress the jump table, and in most cases a 
		single byte is enough to indicate the offset from the base case to all 
		other cases. This leads to problems for static rewriting because first 
		of all jump tables are harder to detect, as on x86 scanning the data 
		sections for arrays of valid instruction pointers was a quite reliable 
		way of detecting jump tables, while on ARM they are indistinguishable 
		from random bytes; secondly inserting too much instrumentation between 
		cases of the same jump table could lead to the offset not fitting into 
		a single byte anymore, and breaking the whole jump table. Finally, 
		extracting the number of cases of a jump table is quite harder in ARM, 
		since it is impossible to scan cases until an invalid pointer is found, 
		as like stated before, jump table entries in ARM are indistinguishable 
		from random bytes.
	\item \emph{Discontinuities in immediates}: Some ARM instructions, like 
		\texttt{ADD}, support having immediates as one of the operands.  
		However, they do not accept a classical range of immediates like in 
		x86, but instead a specific set of values that may not be continuous.  
		For example the \texttt{ADD} instruction can use only immediates that 
		can be expressed with a value of 8 bits scaled by a \texttt{ROR} with a
		4 bits value.
	\item \emph{Not enough mature tools}: The popularity of ARM CPUs is still 
		relatively new and the tooling is not mature enough, as in fact we 
		found bugs in both the disassembler we chose to use (Capstone 
		\cite{capstone}) and the debugger (GDB)
\end{itemize}

%\todo{Get into detail on what are the key differences between x86 and ARM. Give 
%some foreshadowing on the challenges that will be faced because of ARM quirks.}



%%%%%%%%%%%%%%%%
\chapter{Design}
%%%%%%%%%%%%%%%%
%Introduce and discuss the design decisions that you made during this project.
%Highlight why individual decisions are important and/or necessary. Discuss
%how the design fits together.
%This section is usually 5-10 pages.

In this section we will go over the design decisions that we made during the 
development of the ARM version of \texttt{Retrowrite}. We will start from goals 
that we put ourselves at the start of the project, and then follow up with the 
key issues that we encountered caused by the quirks of the ARM architecture. We 
will then go over the solution we adopted for those problems both in the 
\emph{symbolization} and in the \emph{instrumentation} sides of 
\texttt{Retrowrite}. 

\section{Goals}


Our goal is to develop a platform on which it is easily possible to write 
instrumentation passes for ARM binaries.  We identify the following 
requirements that our solution should satisfy:

\begin{itemize}
	\item \textbf{RQ1:} The implementation should supports COTS binaries, and 
		should scale well to large binaries.
	\item \textbf{RQ2:} The symbolization and instrumentation processes must 
		not hinder the intrinsic functionality of the binary being rewritten.
	\item \textbf{RQ3:} The memory safety instrumentation pass we develop must 
		be competitive in performance and functionality with its source-based 
		counterpart.


\end{itemize}

%\todo{Give a rundown of specific goals that we set ourselves at the start of 
%the project:}
%\begin{itemize}
	%\item Study the ARM architecture and the main differences from x86
	%\item Study the codebase of the Retrowrite project, understand the theory 
		%behind it and its implementation
	%\item Write tests to ensure the robustness of the implementation
	%\item Implement and introduce support for ARM in the Retrowrite project
	%\item Develop a sample instrumentation pass (BASAN)
	%\item Evaluate the resulting implementation against state-of-the-art 
		%techniques
%\end{itemize}
	 

\section{Key Issues}
We will now go over the issues we encountered while porting \texttt{Retrowrite} 
to the ARM architecture that we briefly mentioned in the previous chapter.  

\subsection{Fixed size instruction set}
The Aarch64 instruction set is defined as \emph{fixed size}, because every 
instruction is large exactly 4 bytes. This makes the CPU design simpler,
helps keeping memory always aligned, and permits the CPU to fetch multiple 
instructions at once, since decoding is not necessary to determine instruction 
boundaries. However, despite the many advantages of this characteristic, there 
are some drawbacks too, including not being able to store a pointer in a single 
instruction (as pointers have a size of 8 bytes). The Aarch64 ISA provides two 
main solutions to this problem. 

The first one consists in storing the pointers in a special read-only region of 
memory, called a \emph{literal pool}, and then load those pointers into a 
registers using the special construct \texttt{ldr <reg>, =pointer}, a 
pseudo-instruction that the assembler will translate with the correct memory 
address once \texttt{pointer} has been stored in a literal pool.  Since all 
\emph{ldr} instructions are PC-relative, and since there are 21 bits available 
to store the offset from the PC, the assembler will store \emph{pointer} in a 
literal pool which is in the $\pm$ 1MB range of the \texttt{ldr} instruction.  
While this is a simple and straightforward approach, very useful in the case of 
hand-written assembly, this requires an additional memory access that may 
impact performance in the long run. Furthermore, the assembly will fail if it 
is not possible to store a literal pool in the given range, such as in the case 
of a function larger than 2MB. In that case, it is up to the programmer to find 
a suitable spot for the literal pool, by manually specifying its location with 
the \texttt{.ltorg} assembly directive. It is often recommended to store 
literal pools directly after non-conditional jumps to avoid executing over them 
\cite{literalpools}.

The second solution is to build pointers using multiple instructions and basic 
arithmetic primitives. Aarch64 offers instructions such as \texttt{adrp <reg>, 
pointer}, which loads the base page of a pointer into a register. It is a 
PC-relative instruction too, and is able to target pointers in the $\pm$ 4GB 
range. In other words, the \texttt{adrp} instruction can point only to memory 
locations that are 4KB aligned. However the instruction can be followed by an 
\texttt{add}, a \texttt{sub} or even an offset-based load such as \texttt{ldr 
<register>, [<base\_page>, offset]}. This second way, while more contrived and 
certainly harder to read, is actually faster than the first one, as it does not 
require a memory access, and also often benefits from custom hardware 
optimizations (such as in the Cortex A72, one of the most common ARM CPUs 
\cite{pointeroptimizations}). 

\subsubsection{The global variable problem}
For the reasons stated above, compilers generally use the second options, 
preferring performance over assembly readability. This spawns a huge set of 
problems, in particular:  \todo{finish here}

\subsection{Jump Tables}


\begin{itemize}
	\item Alignment issues
	\item Global variables (!)
	\item Jump tables (!!)
	\item Control flow broken by too much instrumentation (e.g. short jumps)
	\item Literal pools (maybe move to only chapter 4, implementation?)
\end{itemize}


\section{Symbolizer}
\todo{Add a figure for the system overview here, with a short explanation on 
how it is split into a single Symbolizer and multiple possible instrumentation 
modules}

\todo{High level rundown of how the original x86 Symbolizer works.
Explain how we approached the symbolizer key issues mentioned above here.}




\section{Instrumentation (ASan)}
\todo{Explain the ASan algorithm, the concept of shadow memory, etc.
Explain the limitations of our binary ASan compared to source ASan:}
\begin{itemize}
	\item No checks on global variables
	\item Checks on the stack only at the stack-frame level
	\item Many more instructions instrumented because of lack of source code
\end{itemize}







%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Implementation}
%%%%%%%%%%%%%%%%%%%%%%%%
%The implementation covers some of the implementation details of your project.
%This is not intended to be a low level description of every line of code that
%you wrote but covers the implementation aspects of the projects.

%This section is usually 3-5 pages.


\section{Symbolizer}
\todo{Go into the nitty-gritty details of my horrible hackish implementation, }

\subsection{Global variables}
\todo{}
\subsubsection{How were they detected}
\subsubsection{How were they fixed}
\subsubsection{The literal pool dilemma}

\subsection{Jump Tables}
\subsubsection{How were they detected}
\todo{Pseudo-emulating ARM binaries to detect jump tables}
\subsubsection{How were they fixed}
\todo{Explain the following:}
\begin{itemize}
	\item List all ideas we had
	\item Explain why we chose to implement the ungodly hack of expanding them 
	\item also, nop padding between cases
\end{itemize}

\section{ASan instrumentation}
\todo{Overview of the ASan algorithm, how it was hand-translated to ARM 
assembly}
\todo{Various optimizations that were introduced to lower instrumentation 
overhead}




\todo{}

%%%%%%%%%%%%%%%%%%%%
\chapter{Evaluation}
%%%%%%%%%%%%%%%%%%%%
%In the evaluation you convince the reader that your design works as intended.
%Describe the evaluation setup, the designed experiments, and how the
%experiments showcase the individual points you want to prove.

%This section is usually 5-10 pages.

\section{Experiment overview}
\todo{Explain our setup, the hardware we used, what we were measuring.}

\todo{Talk a bit on also the difficulties caused by the nature of the 
benchmarks, namely very long times (up to 48 hours, and probably more now that 
I have CloudLab :D ), very high RAM usage, and so on.}

\todo{Talk a bit on how those difficulties were approached, with the very 
strict collection of experiment metadata using Github CI and the telegram bot.}

\section{SPEC CPU}
\todo{Explain how much do I hate the SPEC CPU benchmark, who developed it, the 
crazy fact that it is the accepted state of the art method for benchmarking 
single-core performance, how counter-intuitive their directory structure, 
config files, and bash scripts are.  /s}

\section{Results}
\todo{Present  plots and tables, and a detailed explanation on each one.
Make sure to respect the style of Mathias' previous papers/theses, to avoid 
getting him unnecessarily angry}

\todo{Compare it to state-of-the-art dynamic instrumentation (QEMU).}


%%%%%%%%%%%%%%%%%%%%%%
\chapter{Related Work}
%%%%%%%%%%%%%%%%%%%%%%

%The related work section covers closely related work. Here you can highlight
%the related work, how it solved the problem, and why it solved a different
%problem. Do not play down the importance of related work, all of these
%systems have been published and evaluated! Say what is different and how
%you overcome some of the weaknesses of related work by discussing the 
%trade-offs. Stay positive!
%This section is usually 3-5 pages.


% useful links:
% REPICA: https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8454360
%         good state of the art overview, both ARM64 and arm32
%         1. do not use symbolization, but relative address correction
%         2. Detect jump tables and global pointers, but not fix them
%         3. Overhead is exactly the same as retrowrite
% A SURVEY ON BINARY REWRITING: https://publications.sba-research.org/publications/201906%20-%20GMerzdovnik%20-%20From%20hack%20to%20elaborate%20technique.pdf

{

\setlength{\parindent}{0cm}

\todo{This section now has only list of related work I would like to talk 
about, will be expanded later}



\textbf{Dynamic rewriters in general}:\\
First, the good ol' ones: \texttt{PIN}\cite{pin}, 
\texttt{Valgrind}\cite{valgrind} and \texttt{DynInst}\cite{dyninst}.

More recently, \texttt{Multiverse} \cite{multiverse}, \texttt{Frida} and 
\texttt{DynamorIO} are interesting examples.

\texttt{QASAN}\cite{qasan} is similar to Retrowrite's BAsan, much slower but 
also much more portable (works on any binary supported by QEMU)



\textbf{Static rewriters that use lifting to IR}:\\
\texttt{McSema} \cite{mcsema} is a great example of LLVM IR lifting approach, 
also particularly nice as it supports C++ exceptions

\textbf{Static rewriters that use trampolines}:\\
\texttt{E9patch}\cite{e9patch} uses only trampolines, no need to recover 
control flow, but also noticeable overhead

%\todo{Also say that trampolines are especially effective on ARM because every 
%instruction is always 4 bytes, so you don't need to worry about overwriting 
%instructions shorter than a jmp like in x86}

\textbf{Static rewriters that use symbolization}:\\
\texttt{Uroboros}\cite{uroboros} and \texttt{Ramblr}\cite{ramblr} are the first 
reassemblable assembly approaches (symbolization), \texttt{Retrowrite} 
\cite{dinesh20oakland} (x86 version)

\textbf{Static rewriters aimed at ARM binaries}:\\
\texttt{Repica} \cite{repica} is probably the most recent addition to binary 
rewriters aimed at ARM binaries.  

\todo{Maybe add Bistro ? } \cite{bistro}

For more check out recent surveys on the area of binary rewriting 
\cite{binaryrewritingsurvey}

}

%%%%%%%%%%%%%%%%%%%%%
\chapter{Future Work}
%%%%%%%%%%%%%%%%%%%%%

{

\setlength{\parindent}{0cm}
\hangindent=0.7cm \textbf{Support for more source languages}: For now, 
\texttt{Retrowrite} supports only binaries compiled from the C language, both 
for the x86 and the ARM implementation. The easiest addition would be to add 
support for C++ by expanding the analysis capabilities of \texttt{Retrowrite} 
to support exception tables too, but many more languages could be supported in 
the future. 

\hangindent=0.7cm \textbf{Support for kernel space binaries}: Right now the ARM 
port of \texttt{Retrowrite} supports only userspace binaries, contrary to the 
x86 version that supports linux kernel modules too. The kernel version of 
\texttt{Retrowrite\_ARM} would prove to be particularly interesting as it would 
open new ways to efficiently fuzz Android kernel modules.

\hangindent=0.7cm \textbf{Support for more executable formats/operating 
systems}: The current implementation of the \texttt{Retrowrite} tool is aimed 
only towards ELF files, but adding support for MACH-O and PE binaries should 
not require too much effort. This would also be interesting as Windows and 
MacOS present way more closed-source modules compared to Linux.

\hangindent=0.7cm \textbf{More instrumentation passes}: While right now we 
implemented only the AddressSanitizer instrumentation in the ARM port of
\texttt{Retrowrite}, the design of \texttt{Retrowrite} is modular to make
adding new instrumentation passes or new mitigations very easy for anyone. To
name a few, the interesting ones would be:
\begin{itemize}
	\item Shadow stack (return address protection)
	\item ARM pointer authentication (hardware-assisted)
	\item Control flow authentication
	\item Coverage-guidance for fuzzing
\end{itemize}

}

%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}
%%%%%%%%%%%%%%%%%%%%

%In the conclusion you repeat the main result and finalize the discussion of
%your project. Mention the core results and why as well as how your system
%advances the status quo.


In summary, we develop the ARM architecture implementation of the 
\texttt{Retrowrite} project, a scalable static rewriter for linux C binaries.  
\texttt{Retrowrite} enables targeted application of static instrumentation 
where no source code is available, such as proprietary binaries, inline 
assembly, or code generated by a deprecated compiler.  We also present an 
example instrumentation pass on top of the symbolization engine, 
AddressSanitizer, particularly useful for fuzzing purposes. We present new 
solutions to problems arising from the peculiarities of the ARM architecture 
such as the fixed-size instruction set, global variable accesses and jump table 
instrumentation. We show that the total overhead of the symbolization and the 
instrumentation pass are competitive with source based AddressSanitizer.  Our 
work shows that \texttt{Retrowrite}'s original approach is not limited to the 
x86 architecture, but can be applied to the ARM architecture and more.



\cleardoublepage
\phantomsection

\addcontentsline{toc}{chapter}{Bibliography}
\printbibliography

% Appendices are optional
% \appendix
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \chapter{How to make a transmogrifier}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% In case you ever need an (optional) appendix.
%

\end{document}
